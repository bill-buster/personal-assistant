---
description: LLM provider and ChatModel implementation patterns
globs: ["src/llm/**/*.ts", "src/providers/**/*.ts", "src/embeddings/**/*.ts"]
alwaysApply: false
---

# Provider Implementation Patterns

## ChatModel Interface

All LLM providers implement the ChatModel interface:

```typescript
import type { ToolSpec, Message, TokenUsage } from '../core/types';

export interface ChatModel {
    /** Provider name for logging/debugging */
    readonly name: string;
    
    /** Send a chat completion request */
    chat(request: ChatRequest): Promise<ChatResponse>;
    
    /** Stream a chat completion (optional) */
    chatStream?(request: ChatRequest): AsyncGenerator<ChatChunk, void, unknown>;
    
    /** Check if provider is available/configured */
    isAvailable?(): boolean;
}
```

## ChatRequest Structure

```typescript
export interface ChatRequest {
    prompt: string;
    tools?: Record<string, ToolSpec>;
    history?: Message[];
    systemPrompt?: string;
    options?: ChatOptions;
}

export interface ChatOptions {
    toolFormat?: 'standard' | 'compact';
    verbose?: boolean;
    maxTokens?: number;
    temperature?: number;
}
```

## ChatResponse Structure

```typescript
export interface ChatResponse {
    ok: boolean;
    toolCall?: ToolCall | null;
    reply?: string | null;
    error?: string;
    usage?: TokenUsage | null;
    raw?: unknown;  // For debugging
}
```

## Provider Adapter Pattern

```typescript
// src/llm/providers/groq.ts
export class GroqAdapter implements ChatModel {
    readonly name = 'groq';
    private apiKey: string;
    private model: string;
    
    constructor(config: GroqConfig) {
        this.apiKey = config.apiKey;
        this.model = config.model || 'llama-3.3-70b-versatile';
    }
    
    isAvailable(): boolean {
        return !!this.apiKey;
    }
    
    async chat(request: ChatRequest): Promise<ChatResponse> {
        // Implementation...
    }
}
```

## LLMProvider Interface (Legacy)

For backward compatibility, the runtime also uses:

```typescript
export interface LLMProvider {
    complete(
        prompt: string,
        tools: Record<string, ToolSpec>,
        history?: Message[],
        verbose?: boolean,
        systemPrompt?: string,
        options?: { toolFormat?: 'standard' | 'compact' }
    ): Promise<CompletionResult>;

    completeStream?(
        prompt: string,
        history?: Message[],
        verbose?: boolean,
        systemPrompt?: string
    ): AsyncGenerator<StreamChunk, void, unknown>;
}
```

## EmbeddingModel Interface

```typescript
export interface EmbeddingModel {
    embed(text: string): Promise<number[]>;
    embedBatch?(texts: string[]): Promise<number[][]>;
}
```

## Provider Selection

Providers are selected via config:

```typescript
// Config resolution
const config = resolveConfig(loadConfig());
const provider = config.defaultProvider; // 'groq' | 'openrouter' | 'mock'

// Runtime building
const runtime = buildRuntime(config);
// runtime.provider is the LLMProvider instance
```

## Mock Provider for Testing

```typescript
// src/llm/providers/MockChatModel.ts
export class MockChatModel implements ChatModel {
    readonly name = 'mock';
    
    async chat(request: ChatRequest): Promise<ChatResponse> {
        // Return predictable responses for testing
        return {
            ok: true,
            reply: `Mock response to: ${request.prompt}`,
            usage: { prompt_tokens: 10, completion_tokens: 20, total_tokens: 30 },
        };
    }
    
    isAvailable(): boolean {
        return true;  // Always available
    }
}
```

## Usage Tracking

Track token usage for cost management:

```typescript
export interface TokenUsage {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
}

export type UsageHook = (usage: TokenUsage, model: string) => void;

export interface SpendLimits {
    maxTokensPerRequest?: number;
    maxTokensPerDay?: number;
    maxCostPerDay?: number;
}
```

## Error Handling in Providers

Providers should never throw - always return structured errors:

```typescript
async chat(request: ChatRequest): Promise<ChatResponse> {
    try {
        const response = await fetch(this.endpoint, {
            method: 'POST',
            headers: { 'Authorization': `Bearer ${this.apiKey}` },
            body: JSON.stringify(this.buildPayload(request)),
        });
        
        if (!response.ok) {
            const errorBody = await response.text();
            return {
                ok: false,
                error: `API error ${response.status}: ${errorBody}`,
            };
        }
        
        const data = await response.json();
        return this.parseResponse(data);
        
    } catch (err: any) {
        return {
            ok: false,
            error: `Network error: ${err.message}`,
        };
    }
}
```

## Retry Pattern

For transient failures, implement exponential backoff:

```typescript
async chatWithRetry(
    request: ChatRequest, 
    maxRetries: number = 3
): Promise<ChatResponse> {
    let lastError: string = '';
    
    for (let attempt = 0; attempt < maxRetries; attempt++) {
        const result = await this.chat(request);
        
        if (result.ok) return result;
        
        // Only retry on transient errors
        if (this.isTransientError(result.error)) {
            lastError = result.error || 'Unknown error';
            const delay = Math.pow(2, attempt) * 1000; // 1s, 2s, 4s
            await new Promise(r => setTimeout(r, delay));
            continue;
        }
        
        // Non-transient error, don't retry
        return result;
    }
    
    return { ok: false, error: `Failed after ${maxRetries} attempts: ${lastError}` };
}

private isTransientError(error?: string): boolean {
    if (!error) return false;
    return error.includes('429') ||      // Rate limited
           error.includes('503') ||      // Service unavailable
           error.includes('timeout') ||
           error.includes('ECONNRESET');
}
```

## Rate Limiting

Respect provider rate limits:

```typescript
class RateLimiter {
    private lastCall: number = 0;
    private minInterval: number;  // ms between calls
    
    constructor(requestsPerSecond: number) {
        this.minInterval = 1000 / requestsPerSecond;
    }
    
    async wait(): Promise<void> {
        const now = Date.now();
        const elapsed = now - this.lastCall;
        if (elapsed < this.minInterval) {
            await new Promise(r => setTimeout(r, this.minInterval - elapsed));
        }
        this.lastCall = Date.now();
    }
}

// Usage in provider
private rateLimiter = new RateLimiter(10); // 10 req/sec

async chat(request: ChatRequest): Promise<ChatResponse> {
    await this.rateLimiter.wait();
    // ... make request
}
```

## Timeout Handling

Set reasonable timeouts:

```typescript
async chat(request: ChatRequest): Promise<ChatResponse> {
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort(), 30000); // 30s
    
    try {
        const response = await fetch(this.endpoint, {
            signal: controller.signal,
            // ...
        });
        return this.parseResponse(await response.json());
    } catch (err: any) {
        if (err.name === 'AbortError') {
            return { ok: false, error: 'Request timed out after 30s' };
        }
        return { ok: false, error: err.message };
    } finally {
        clearTimeout(timeout);
    }
}
```
