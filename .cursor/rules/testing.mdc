---
description: Testing patterns and conventions
globs: ["**/*.test.ts", "**/run_tests.ts"]
alwaysApply: false
---

# Testing Conventions

## ⚠️ BEFORE Running Tests

**Check existing test results first** to avoid wasting resources:

```bash
# Check if tests already passed recently
cat .test-results/latest.json 2>/dev/null

# Or check specific test
grep "executor.test" .test-results/latest.json
```

If tests passed within the last hour and no relevant files changed, **skip re-running**.

## Test Result Persistence

Always save test results for other agents to access:

```typescript
// At end of test file, save results
const testResult = {
    file: path.basename(__filename),
    timestamp: new Date().toISOString(),
    passed: failures === 0,
    failures,
    duration_ms: Date.now() - startTime,
    filesHashed: ['src/core/executor.ts', 'src/tools/memory_tools.ts'], // relevant source files
};

// Append to shared results file
const resultsDir = path.join(__dirname, '..', '.test-results');
if (!fs.existsSync(resultsDir)) fs.mkdirSync(resultsDir, { recursive: true });
fs.appendFileSync(
    path.join(resultsDir, 'results.jsonl'),
    JSON.stringify(testResult) + '\n'
);
```

## Test Results Location

```
.test-results/
├── results.jsonl      # Append-only log of all test runs
├── latest.json        # Summary of most recent full test run
└── checksums.json     # File hashes to detect changes
```

## Checking Before Running

Before running tests, check if they're needed:

```typescript
function shouldRunTest(testFile: string): boolean {
    const resultsPath = '.test-results/results.jsonl';
    if (!fs.existsSync(resultsPath)) return true;
    
    // Find last result for this test
    const lines = fs.readFileSync(resultsPath, 'utf8').trim().split('\n');
    for (let i = lines.length - 1; i >= 0; i--) {
        const result = JSON.parse(lines[i]);
        if (result.file === testFile && result.passed) {
            // Check if source files changed since last run
            const lastRun = new Date(result.timestamp).getTime();
            const hourAgo = Date.now() - 3600000;
            if (lastRun > hourAgo) {
                console.log(`⏭️  Skipping ${testFile} - passed recently`);
                return false;
            }
        }
    }
    return true;
}
```

## Test File Location

Tests are colocated with source files:

```
src/
├── executor.test.ts      # Tests for core/executor.ts
├── router.test.ts        # Tests for app/router.ts
├── memory_store.test.ts  # Tests for storage/memory_store.ts
└── tools/
    └── comms_tools.test.ts
```

## Test Structure

Tests use a custom runner with simple assertions:

```typescript
#!/usr/bin/env node

import * as fs from 'node:fs';
import * as path from 'node:path';
import { spawnSync } from 'node:child_process';

// Setup isolated test directory
const testRootRaw = fs.mkdtempSync(path.join(os.tmpdir(), 'my-test-'));
const testRoot = fs.realpathSync(testRootRaw);

let failures = 0;

try {
    // Test case
    const result = runExecutor(payload);
    const json = parseOutput(result.stdout);
    
    if (!json || json.ok !== true) {
        failures += 1;
        logLine('FAIL\ncase: test name\nexpected: ok true\n\n', process.stderr);
    }
    
    // More test cases...
    
} finally {
    // Cleanup
    fs.rmSync(testRoot, { recursive: true, force: true });
}

if (failures > 0) {
    process.exit(1);
}

logLine('RESULT\nstatus: OK\n', process.stdout);
export { };  // Make it a module
```

## Test Environment Setup

Always isolate test data:

```typescript
// Create isolated temp directory
const testRootRaw = fs.mkdtempSync(path.join(os.tmpdir(), 'executor-test-'));
const testRoot = fs.realpathSync(testRootRaw);

const testDataDir = path.join(testRoot, 'data');
const testConfigDir = path.join(testRoot, 'config');
fs.mkdirSync(testDataDir, { recursive: true });
fs.mkdirSync(testConfigDir, { recursive: true });

// Set environment
const testEnv: NodeJS.ProcessEnv = {
    ...process.env,
    ASSISTANT_CONFIG_DIR: testConfigDir,
    ASSISTANT_DATA_DIR: testDataDir,
};
```

## Running Executors in Tests

Use spawnSync to run executors as separate processes:

```typescript
function runExecutor(payload: any) {
    const result = spawnSync(process.execPath, baseArgs, {
        input: JSON.stringify(payload),
        cwd: spikeDir,
        encoding: 'utf8',
        env: testEnv,
    });
    return {
        status: result.status,
        stdout: (result.stdout || '').trim(),
    };
}
```

## Parsing Test Output

Handle JSON parsing robustly:

```typescript
function parseOutput(output: string) {
    try {
        return JSON.parse(output);
    } catch (err) {
        // Try to find the last valid JSON line
        const lines = output.trim().split('\n');
        for (let i = lines.length - 1; i >= 0; i--) {
            try {
                const json = JSON.parse(lines[i]);
                if (json && (typeof json.ok === 'boolean')) {
                    return json;
                }
            } catch (e) {
                // continue
            }
        }
        return null;
    }
}
```

## Test Assertions

Simple assertion pattern with logging:

```typescript
if (!json || json.ok !== true || json.result.content !== 'expected') {
    failures += 1;
    logLine('FAIL\ncase: descriptive test name\nexpected: specific expectation\n\n', process.stderr);
}

// For error code checks
if (!json || !json.error || json.error.code !== 'EXPECTED_ERROR_CODE') {
    failures += 1;
    logLine('FAIL\ncase: error code check\nexpected: EXPECTED_ERROR_CODE\n\n', process.stderr);
}
```

## Debug Info Validation

Helper for checking _debug objects:

```typescript
function checkDebug(debug: any, expected: any) {
    const keys = ['path', 'duration_ms', 'model', 'memory_read', 'memory_write'];
    if (!debug || typeof debug !== 'object') {
        failures += 1;
        logLine('FAIL\ncase: debug\nexpected: object\n\n', process.stderr);
        return;
    }
    for (const key of keys) {
        if (!(key in debug)) {
            failures += 1;
            logLine(`FAIL\ncase: debug\nmissing: ${key}\n\n`, process.stderr);
            return;
        }
    }
}
```

## Running Tests

```bash
# Full test suite
npm test

# Single test file (build first)
npm run build && TEST_DIST=1 node dist/run_tests.js
```

## Agent Workflow for Tests

When an agent needs to verify code changes:

### 1. Check Existing Results First
```bash
# Read the latest test results
cat .test-results/latest.json

# If all passed and no relevant changes, STOP HERE
```

### 2. Run Only Affected Tests
```bash
# If you changed src/tools/memory_tools.ts, only run:
npm run build && node dist/memory_store.test.js

# NOT the full suite
```

### 3. Save Results After Running
```bash
# The test should auto-save, but verify:
tail -1 .test-results/results.jsonl
```

### 4. Update Summary for Next Agent
```bash
# After full test run, update latest.json
echo '{"timestamp":"'$(date -Iseconds)'","passed":true,"tests":42}' > .test-results/latest.json
```

## Test Result Schema

```typescript
interface TestResult {
    file: string;           // e.g., "executor.test.ts"
    timestamp: string;      // ISO 8601
    passed: boolean;
    failures: number;
    duration_ms: number;
    filesHashed?: string[]; // Source files this test covers
    error?: string;         // If failed, why
}

interface TestSummary {
    timestamp: string;
    passed: boolean;
    total: number;
    failed: number;
    skipped: number;
    duration_ms: number;
}
```

## Avoiding Redundant Runs

**DO NOT** run full test suite if:
- `.test-results/latest.json` shows all passed
- No source files changed since `timestamp`
- Less than 1 hour since last run

**DO** run tests if:
- You modified source code
- You modified test files
- No recent results exist
- Previous run failed

## Sharing Context Between Agents

When handing off to another agent, include:

```markdown
## Test Status
- Last run: 2024-01-05T10:30:00Z
- Result: ✅ All 42 tests passed
- Duration: 12.3s
- Changed files since: none

No need to re-run tests.
```

## Test Coverage Expectations

### Every Tool Must Test

1. **Success case** - Normal operation works
2. **Validation errors** - Invalid args rejected
3. **Permission denied** - Blocked operations fail correctly
4. **Edge cases** - Empty input, max size, etc.

```typescript
// Example: minimum tests for a tool
// T1: Success case
const successPayload = { tool_call: { tool_name: 'my_tool', args: { text: 'valid' } } };
// Verify: ok === true, result matches expected

// T2: Missing required arg
const missingArgPayload = { tool_call: { tool_name: 'my_tool', args: {} } };
// Verify: ok === false, error.code === 'MISSING_ARGUMENT'

// T3: Invalid arg type
const invalidTypePayload = { tool_call: { tool_name: 'my_tool', args: { text: 123 } } };
// Verify: ok === false, error.code === 'INVALID_ARGUMENT'

// T4: Permission denied (if applicable)
// Verify: ok === false, error.code === 'DENIED_*'
```

### Integration Tests

Test full flows, not just units:

```typescript
// Integration: remember → recall flow
runExecutor({ tool_call: { tool_name: 'remember', args: { text: 'test data' } } });
const recall = runExecutor({ tool_call: { tool_name: 'recall', args: { query: 'test' } } });
// Verify: recall finds the remembered data
```

## Test Naming Conventions

Use descriptive case names:

```typescript
// Good - describes what's being tested
'FAIL\ncase: read_file missing file returns EXEC_ERROR\n'
'FAIL\ncase: task_add with empty text returns MISSING_ARGUMENT\n'
'FAIL\ncase: run_cmd with blocked command returns DENIED_COMMAND_ALLOWLIST\n'

// Bad - too vague
'FAIL\ncase: test1\n'
'FAIL\ncase: error case\n'
```

## Test Categories

Organize tests by category:

```typescript
// 1. Unit Tests - Single function/module
//    File: src/parsers/task_parser.test.ts
//    Tests: parseTaskCommand() returns correct tool/args

// 2. Integration Tests - Multiple components
//    File: src/executor.test.ts
//    Tests: Full tool execution through Executor

// 3. E2E Tests - CLI to result
//    File: src/cli.test.ts (if exists)
//    Tests: CLI commands produce correct output

// 4. Security Tests
//    File: src/permissions.test.ts
//    Tests: Blocked paths/commands fail correctly
```

## When to Write New Tests

**Always write tests when:**
- Adding a new tool
- Fixing a bug (test for the bug first)
- Changing security-related code
- Modifying validation logic

**Tests are optional for:**
- Documentation changes
- Refactoring with no behavior change (existing tests should pass)
- Adding logging/debug output
