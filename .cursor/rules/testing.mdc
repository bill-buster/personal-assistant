---
description: Testing patterns and conventions
globs: ["**/*.test.ts", "**/run_tests.ts"]
alwaysApply: false
---

# Testing Conventions

## Test Execution Policy

**See `.cursor/rules/test_policy.mdc` for when to run tests and how to avoid redundant runs.**

The test runner automatically handles result persistence and caching via `TestCache` in `src/core/test_cache.ts`. You don't need to write persistence code in test files.

## Test Results Location

The test runner automatically creates:

```
.test-results/
├── results.jsonl      # Append-only log of all test runs (auto-managed)
├── latest.json        # Summary of most recent full test run (auto-managed)
└── checksums.json     # File hashes for change detection (auto-managed)
```

These files are managed by `src/run_tests.ts` and `src/core/test_cache.ts`. Do not manually write to them.

## Test File Location

Tests are colocated with source files:

```
src/
├── executor.test.ts      # Tests for core/executor.ts
├── router.test.ts        # Tests for app/router.ts
├── memory_store.test.ts  # Tests for storage/memory_store.ts
└── tools/
    └── comms_tools.test.ts
```

## Test Structure

Tests use a custom runner with simple assertions:

```typescript
#!/usr/bin/env node

import * as fs from 'node:fs';
import * as path from 'node:path';
import { spawnSync } from 'node:child_process';

// Setup isolated test directory
const testRootRaw = fs.mkdtempSync(path.join(os.tmpdir(), 'my-test-'));
const testRoot = fs.realpathSync(testRootRaw);

let failures = 0;

try {
    // Test case
    const result = runExecutor(payload);
    const json = parseOutput(result.stdout);
    
    if (!json || json.ok !== true) {
        failures += 1;
        logLine('FAIL\ncase: test name\nexpected: ok true\n\n', process.stderr);
    }
    
    // More test cases...
    
} finally {
    // Cleanup
    fs.rmSync(testRoot, { recursive: true, force: true });
}

if (failures > 0) {
    process.exit(1);
}

logLine('RESULT\nstatus: OK\n', process.stdout);
export { };  // Make it a module
```

## Test Environment Setup

Always isolate test data:

```typescript
// Create isolated temp directory
const testRootRaw = fs.mkdtempSync(path.join(os.tmpdir(), 'executor-test-'));
const testRoot = fs.realpathSync(testRootRaw);

const testDataDir = path.join(testRoot, 'data');
const testConfigDir = path.join(testRoot, 'config');
fs.mkdirSync(testDataDir, { recursive: true });
fs.mkdirSync(testConfigDir, { recursive: true });

// Set environment
const testEnv: NodeJS.ProcessEnv = {
    ...process.env,
    ASSISTANT_CONFIG_DIR: testConfigDir,
    ASSISTANT_DATA_DIR: testDataDir,
};
```

## Running Executors in Tests

Use spawnSync to run executors as separate processes:

```typescript
function runExecutor(payload: any) {
    const result = spawnSync(process.execPath, baseArgs, {
        input: JSON.stringify(payload),
        cwd: spikeDir,
        encoding: 'utf8',
        env: testEnv,
    });
    return {
        status: result.status,
        stdout: (result.stdout || '').trim(),
    };
}
```

## Parsing Test Output

Handle JSON parsing robustly:

```typescript
function parseOutput(output: string) {
    try {
        return JSON.parse(output);
    } catch (err) {
        // Try to find the last valid JSON line
        const lines = output.trim().split('\n');
        for (let i = lines.length - 1; i >= 0; i--) {
            try {
                const json = JSON.parse(lines[i]);
                if (json && (typeof json.ok === 'boolean')) {
                    return json;
                }
            } catch (e) {
                // continue
            }
        }
        return null;
    }
}
```

## Test Assertions

Simple assertion pattern with logging:

```typescript
if (!json || json.ok !== true || json.result.content !== 'expected') {
    failures += 1;
    logLine('FAIL\ncase: descriptive test name\nexpected: specific expectation\n\n', process.stderr);
}

// For error code checks
if (!json || !json.error || json.error.code !== 'EXPECTED_ERROR_CODE') {
    failures += 1;
    logLine('FAIL\ncase: error code check\nexpected: EXPECTED_ERROR_CODE\n\n', process.stderr);
}
```

## Debug Info Validation

Helper for checking _debug objects:

```typescript
function checkDebug(debug: any, expected: any) {
    const keys = ['path', 'duration_ms', 'model', 'memory_read', 'memory_write'];
    if (!debug || typeof debug !== 'object') {
        failures += 1;
        logLine('FAIL\ncase: debug\nexpected: object\n\n', process.stderr);
        return;
    }
    for (const key of keys) {
        if (!(key in debug)) {
            failures += 1;
            logLine(`FAIL\ncase: debug\nmissing: ${key}\n\n`, process.stderr);
            return;
        }
    }
}
```

## Running Tests

```bash
# Full test suite
npm test

# Single test file (build first)
npm run build && TEST_DIST=1 node dist/run_tests.js
```

## Running Tests

**See `.cursor/rules/test_policy.mdc` for the complete decision tree on when to run tests.**

Quick reference:
- Check `.test-results/latest.json` first
- Prefer `npm run test:single <name>` for targeted runs
- The test runner automatically handles caching and result persistence

## Test Result Schema

```typescript
interface TestResult {
    file: string;           // e.g., "executor.test.ts"
    timestamp: string;      // ISO 8601
    passed: boolean;
    failures: number;
    duration_ms: number;
    filesHashed?: string[]; // Source files this test covers
    error?: string;         // If failed, why
}

interface TestSummary {
    timestamp: string;
    passed: boolean;
    total: number;
    failed: number;
    skipped: number;
    duration_ms: number;
}
```

## Sharing Context Between Agents

When handing off to another agent, include test status:

```markdown
## Test Status
- Last run: 2024-01-05T10:30:00Z
- Result: ✅ All 42 tests passed
- Duration: 12.3s
- Changed files since: none

No need to re-run tests.
```

**See `.cursor/rules/test_policy.mdc` for complete anti-thrash rules.**

## Test Coverage Expectations

### Every Tool Must Test

1. **Success case** - Normal operation works
2. **Validation errors** - Invalid args rejected
3. **Permission denied** - Blocked operations fail correctly
4. **Edge cases** - Empty input, max size, etc.

```typescript
// Example: minimum tests for a tool
// T1: Success case
const successPayload = { tool_call: { tool_name: 'my_tool', args: { text: 'valid' } } };
// Verify: ok === true, result matches expected

// T2: Missing required arg
const missingArgPayload = { tool_call: { tool_name: 'my_tool', args: {} } };
// Verify: ok === false, error.code === 'MISSING_ARGUMENT'

// T3: Invalid arg type
const invalidTypePayload = { tool_call: { tool_name: 'my_tool', args: { text: 123 } } };
// Verify: ok === false, error.code === 'INVALID_ARGUMENT'

// T4: Permission denied (if applicable)
// Verify: ok === false, error.code === 'DENIED_*'
```

### Integration Tests

Test full flows, not just units:

```typescript
// Integration: remember → recall flow
runExecutor({ tool_call: { tool_name: 'remember', args: { text: 'test data' } } });
const recall = runExecutor({ tool_call: { tool_name: 'recall', args: { query: 'test' } } });
// Verify: recall finds the remembered data
```

## Test Naming Conventions

Use descriptive case names:

```typescript
// Good - describes what's being tested
'FAIL\ncase: read_file missing file returns EXEC_ERROR\n'
'FAIL\ncase: task_add with empty text returns MISSING_ARGUMENT\n'
'FAIL\ncase: run_cmd with blocked command returns DENIED_COMMAND_ALLOWLIST\n'

// Bad - too vague
'FAIL\ncase: test1\n'
'FAIL\ncase: error case\n'
```

## Test Categories

Organize tests by category:

```typescript
// 1. Unit Tests - Single function/module
//    File: src/parsers/task_parser.test.ts
//    Tests: parseTaskCommand() returns correct tool/args

// 2. Integration Tests - Multiple components
//    File: src/executor.test.ts
//    Tests: Full tool execution through Executor

// 3. E2E Tests - CLI to result
//    File: src/cli.test.ts (if exists)
//    Tests: CLI commands produce correct output

// 4. Security Tests
//    File: src/permissions.test.ts
//    Tests: Blocked paths/commands fail correctly
```

## When to Write New Tests

**Always write tests when:**
- Adding a new tool
- Fixing a bug (test for the bug first)
- Changing security-related code
- Modifying validation logic

**Tests are optional for:**
- Documentation changes
- Refactoring with no behavior change (existing tests should pass)
- Adding logging/debug output
